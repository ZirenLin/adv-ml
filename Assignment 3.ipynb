{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "github Link: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)  Visualize the categories of your target variable and describe the dataset generally (the data includes news articles from the BBC news.)  A simple description is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2225 entries, 0 to 2224\n",
      "Data columns (total 2 columns):\n",
      "category    2225 non-null object\n",
      "text        2225 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 34.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvFvnyVgAAEodJREFUeJzt3X+QZWV95/H3R0YxK4Thx2SKzKBt6WRddrc02IUYk6yRxChuArsBo5vVWUJqNgn+imsSks2aWGUqqIkYdxMNGyxHoyISE0ZgjewgUVGQngAzCFEnCIFZdFoU8sNVg37zx3naudNMT9+e7qabh/er6tZ9znOec89znnvO554+90enqpAk9etRK90BSdLyMuglqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnVuz0h0AOO6442piYmKluyFJDys7duz4clWtm6/dqgj6iYkJpqamVrobkvSwkuTOcdp56UaSOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjq3Kr4ZuxQmzrtipbvAHee/YKW7ADgWkvY31hl9kjuS7EpyU5KpVndMkquSfL7dH93qk+StSXYn2ZnkpOXcAEnSwS3k0s2PVNXTqmqyTZ8HbK+qTcD2Ng3wfGBTu20B3rZUnZUkLdxirtGfDmxt5a3AGSP176rBdcDaJMcvYj2SpEUYN+gL+EiSHUm2tLr1VXVPK38RWN/KG4C7Rpa9u9XtJ8mWJFNJpqanpw+h65KkcYz7ZuwPVtWeJN8DXJXkr0dnVlUlqYWsuKouBC4EmJycXNCykqTxjXVGX1V72v1e4M+Ak4EvzVySafd7W/M9wAkji29sdZKkFTBv0Cd5XJIjZ8rAc4FbgG3A5tZsM3BZK28DXto+fXMKcP/IJR5J0kNsnEs364E/SzLT/r1V9eEkNwCXJDkHuBN4YWt/JXAasBv4GnD2kvdakjS2eYO+qm4HnnqA+nuBUw9QX8C5S9I7SdKi+RMIktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6tyale6AtJwmzrtipbvAHee/YKW7oEc4z+glqXMGvSR1buygT3JYkhuTXN6mn5jk+iS7k7w/yWNa/eFtenebP7E8XZckjWMhZ/SvBG4bmX4DcEFVPRn4KnBOqz8H+Gqrv6C1kyStkLGCPslG4AXAH7fpAM8BLm1NtgJntPLpbZo2/9TWXpK0Asb91M1bgF8BjmzTxwL3VdUDbfpuYEMrbwDuAqiqB5Lc39p/eUl6LOmQ+AmkR655z+iT/Htgb1XtWMoVJ9mSZCrJ1PT09FI+tCRpxDiXbp4F/GSSO4CLGS7Z/D6wNsnMXwQbgT2tvAc4AaDNPwq4d/aDVtWFVTVZVZPr1q1b1EZIkuY2b9BX1a9V1caqmgBeBFxdVT8DfBQ4szXbDFzWytvaNG3+1VVVS9prSdLYFvM5+l8FXp1kN8M1+Ita/UXAsa3+1cB5i+uiJGkxFvQTCFV1DXBNK98OnHyANl8HzlqCvkmSloC/dSPpEeeR9gkkfwJBkjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOzRv0SR6b5NNJbk7ymSSva/VPTHJ9kt1J3p/kMa3+8Da9u82fWN5NkCQdzDhn9N8AnlNVTwWeBjwvySnAG4ALqurJwFeBc1r7c4CvtvoLWjtJ0gqZN+hr8A9t8tHtVsBzgEtb/VbgjFY+vU3T5p+aJEvWY0nSgox1jT7JYUluAvYCVwF/A9xXVQ+0JncDG1p5A3AXQJt/P3DsAR5zS5KpJFPT09OL2wpJ0pzGCvqq+lZVPQ3YCJwMPGWxK66qC6tqsqom161bt9iHkyTNYUGfuqmq+4CPAs8E1iZZ02ZtBPa08h7gBIA2/yjg3iXprSRpwcb51M26JGtb+buAHwNuYwj8M1uzzcBlrbytTdPmX11VtZSdliSNb838TTge2JrkMIYXhkuq6vIktwIXJ3k9cCNwUWt/EfDuJLuBrwAvWoZ+S5LGNG/QV9VO4PsPUH87w/X62fVfB85akt5JkhbNb8ZKUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktS5eYM+yQlJPprk1iSfSfLKVn9MkquSfL7dH93qk+StSXYn2ZnkpOXeCEnS3MY5o38A+G9VdSJwCnBukhOB84DtVbUJ2N6mAZ4PbGq3LcDblrzXkqSxzRv0VXVPVf1VK/89cBuwATgd2NqabQXOaOXTgXfV4DpgbZLjl7znkqSxLOgafZIJ4PuB64H1VXVPm/VFYH0rbwDuGlns7lYnSVoBYwd9kiOAPwVeVVV/Nzqvqgqohaw4yZYkU0mmpqenF7KoJGkBxgr6JI9mCPn3VNUHW/WXZi7JtPu9rX4PcMLI4htb3X6q6sKqmqyqyXXr1h1q/yVJ8xjnUzcBLgJuq6o3j8zaBmxu5c3AZSP1L22fvjkFuH/kEo8k6SG2Zow2zwJeAuxKclOr+3XgfOCSJOcAdwIvbPOuBE4DdgNfA85e0h5LkhZk3qCvqk8AmWP2qQdoX8C5i+yXJGmJ+M1YSeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUufmDfok70iyN8ktI3XHJLkqyefb/dGtPknemmR3kp1JTlrOzkuS5jfOGf07gefNqjsP2F5Vm4DtbRrg+cCmdtsCvG1puilJOlTzBn1VfQz4yqzq04GtrbwVOGOk/l01uA5Ym+T4peqsJGnhDvUa/fqquqeVvwisb+UNwF0j7e5udQ+SZEuSqSRT09PTh9gNSdJ8Fv1mbFUVUIew3IVVNVlVk+vWrVtsNyRJczjUoP/SzCWZdr+31e8BThhpt7HVSZJWyKEG/TZgcytvBi4bqX9p+/TNKcD9I5d4JEkrYM18DZK8D3g2cFySu4HfBM4HLklyDnAn8MLW/ErgNGA38DXg7GXosyRpAeYN+qp68RyzTj1A2wLOXWynJElLx2/GSlLnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUuWUJ+iTPS/LZJLuTnLcc65AkjWfJgz7JYcAfAM8HTgRenOTEpV6PJGk8y3FGfzKwu6pur6pvAhcDpy/DeiRJY1iOoN8A3DUyfXerkyStgFTV0j5gcibwvKr6uTb9EuAZVfWyWe22AFva5L8EPrukHTk0xwFfXulOrBKOxcBx2Mex2Ge1jMUTqmrdfI3WLMOK9wAnjExvbHX7qaoLgQuXYf2HLMlUVU2udD9WA8di4Djs41js83Abi+W4dHMDsCnJE5M8BngRsG0Z1iNJGsOSn9FX1QNJXgb8BXAY8I6q+sxSr0eSNJ7luHRDVV0JXLkcj73MVtWlpBXmWAwch30ci30eVmOx5G/GSpJWF38CQZI6Z9AvQJJnJ/mBle7HbEkmktyyyMf43iSXLlWfHq6SXJNkspWvTLK23X5xpE1XYzV7+xa47DvbR6pXnSRnHMq38sc9zpP85Er9xMtCnzODfkxJ1gDPBlZd0C+Fqvp/VbUqD9iVUlWnVdV9wFrgF0fqexur/bavI2cw/AzL2BZynFfVtqo6/9C6tmgLe86qqssb8DjgCuBm4Bbgp4E7gDcCu4BPA09ubSeAq4GdwHbg8a3+ncDbgeuBDwJfZPhOwE3AD630No5s6wTw18B7gNuAS4F/0bb3uNZmErimlf9d24abgBuBI9tj3NLm/5e2vR8GPg+8cWRdzwU+BfwV8AHgiFZ/PnBrG8PfbXVntbG/GfjYKhubU9u27wLeARze2l8DTLbyHQxfjLkY+P9tvN40a6wOA363bedO4OVzjcdqvR1g+36Z4WPSO4HXjbR7aau7GXj3yDHyVuCTwO3Amcvc1//cjt2bgD9q4/8PwG+3fl0HrGcI6q8AX2htn9RuHwZ2AB8HnjKyDXMe58BPtHk3Av8XWD9ynPyvg40Dw4vGXwKXtfrzgZ9p27ALeFJrtw740zbuNwDPavW/1fbPa9ryrzjQczbvuK30TraMO8RPAf97ZPqoduD+95Gd9vJW/hCwuZV/FvjzkSfvcuCwkUF/zUpv2wG2dQKokZ3jHcBrmDvoPzTS9giGT19NsH/Q397G7LHAnQxfgjsO+BjwuNbuV4HXAscyfLN55s39te1+F7BhtG6VjM1vMPxMx/e1uncBr2rla3hw0H9nbEYec2asfoHhxWNNmz5mrvFYrbdZ2/Nchk+UhOEv/suBHwb+NfC5kf3pmJFj5AOt7YkMv3O1XP38V23ffXSb/kOG47iAn2h1bwR+Y6RvZ44svx3Y1MrPAK4eaTfncQ4cPfJc/hzweyPHyWjQP2gcGIL+PuB44HCGF5DXtXmvBN7Syu8FfrCVHw/cNtKXT7ZljwPuBR49e5+c77YsH69cJXYBv5fkDQyB/vEkAO9r898HXNDKzwT+Yyu/m2FnmfGBqvrWQ9Dfxbqrqq5t5T8BXnGQttcCb07yHuCDVXV3G5tR26vqfoAktwJPYPhz8UTg2tb+MQxn9/cDXwcuSnI5w0Ezs553JrmE4Uxppcwem/8BfKGqPtfqtgLnAm85hMf+UeDtVfUAQFV9pf35f6DxeDh4brvd2KaPADYBT2U4Fr4Mw3aOLPPnVfVt4NYk65exb6cCTwduaPvfdwF7gW+yb4x3AD82e8EkRzCc5X9gZF8/fKTJwY7zjcD7kxzPsM9/YY52c43DDVV1T+vH3wAfafW7gB9p5R8FThzp23e3PgNcUVXfAL6RZC/DXywL0m3QV9XnkpwEnAa8Psn2mVmjzcZ4qH9c8s4tj9nbUsAD7Hsf5rHfmVF1fpIrGMbm2iQ/zhBMo74xUv4Ww74S4KqqevHslSc5meFAPBN4GfCcqvr5JM8AXgDsSPL0qrr3UDdwEWaPzX0MZ93Ls7LhS4MPGo/lWt8SC/A7VfVH+1UmLz/IMqP7yoPOGJZQgK1V9Wv7VSavqXb6y759dbZHAfdV1dPmeOyDHef/E3hzVW1L8myGs+wDmWscRuu/PTL97ZG+Pgo4par2Ow5b8B/oWFyQbt+MTfK9wNeq6k8Yrjue1Gb99Mj9p1r5kww/1QDD9bOPz/Gwf89wPXs1enySZ7byfwI+wXDp4emt7qdmGiZ5UlXtqqo3MFwPfMqY67gOeFaSJ7fHeVyS72tnHkfV8EW5X2I4+5tZz/VV9Vpgmv1/A+mhNHtspoCJme0AXsJwHXUuB3verwL+azuLJ8kxc43HKja6fX8B/OzM2WSSDUm+h+E9rLOSHNvqj1mBfm4Hzmz9mRnrJxyk/Xe2q6r+DvhCkrPaskky1/My+/k+in2/17V5Ef0/mI8A33kxTTLXC9KMBWVRt0EP/Fvg00luAn4TeH2rPzrJTobrY7/U6l4OnN3qX9LmHciHgP+Q5KYkP7R8XT8knwXOTXIbwzXFtwGvA34/yRTDmcCMVyW5pW3vPwH/Z5wVVNU0w3XJ97VlP8XwInEkcHmr+wTw6rbIm5Lsah/9/CTDm2UrYfbYXACczfBn/C6GM6u3z7Vw+yvk2jZmb5o1+4+BvwV2JrmZ4YVkrvFYlUa3j+Gyx3uBT7WxuRQ4soafMflt4C/bdr55Bfp5K8P7Kx9pY3sVw7XvuVwM/HKSG5M8ieEk7pzW/88w9//JmH2c/xbDvrKD5fvFylcAk0l2tkulP3+wxvPskw/yiPpmbJI7GN5oWw0/L6qHQJIJhvdo/s0Kd0VaMT2f0UuSeISd0UvSI5Fn9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalz/wxxgu/Ig7vEZwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df['category'].value_counts().plot(kind='bar')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five categories of the target variable 'category': sport, business, politics, tech and entertainment. Sport and business are the two largest categories with around 500 data, while politics, tech and entertainment have smaller data of around 400. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Preprocess your data such that each document in the data is represented as a sequence of equal length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1668,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X=df['text']\n",
    "y=df['category']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state = 0)\n",
    "print X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['politics'],\n",
       "       ['business'],\n",
       "       ['tech'],\n",
       "       ...,\n",
       "       ['business'],\n",
       "       ['entertainment'],\n",
       "       ['tech']], dtype=object)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26566 unique tokens.\n",
      "('Shape of data tensor:', (1668, 500))\n",
      "('Shape of label tensor:', (1668, 5))\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "maxlen = 500\n",
    "training_samples = 1400\n",
    "validation_samples = 300\n",
    "max_words = 2500\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences = tokenizer.texts_to_sequences(X_train) \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "one_hot_enc = OneHotEncoder()\n",
    "one_hot_enc.fit(y_train.values.reshape(-1, 1))\n",
    "labels = one_hot_enc.transform(y_train.values.reshape(-1, 1)).toarray()\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train_train = data[:training_samples] #200 words\n",
    "y_train_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3)  Use the data to fit separate models to each of the following architectures:\n",
    "### A. A model with an embedding layer and dense layers (but w/ no layers meant for sequential data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_34 (Embedding)     (None, 500, 8)            20000     \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 4000)              0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 5)                 20005     \n",
      "=================================================================\n",
      "Total params: 40,005\n",
      "Trainable params: 40,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1120 samples, validate on 280 samples\n",
      "Epoch 1/10\n",
      "1120/1120 [==============================] - 0s 185us/step - loss: 0.6838 - acc: 0.6482 - val_loss: 0.6737 - val_acc: 0.7493\n",
      "Epoch 2/10\n",
      "1120/1120 [==============================] - 0s 71us/step - loss: 0.6628 - acc: 0.7646 - val_loss: 0.6468 - val_acc: 0.7857\n",
      "Epoch 3/10\n",
      "1120/1120 [==============================] - 0s 70us/step - loss: 0.6299 - acc: 0.7873 - val_loss: 0.6065 - val_acc: 0.7950\n",
      "Epoch 4/10\n",
      "1120/1120 [==============================] - 0s 68us/step - loss: 0.5883 - acc: 0.7961 - val_loss: 0.5651 - val_acc: 0.7957\n",
      "Epoch 5/10\n",
      "1120/1120 [==============================] - 0s 87us/step - loss: 0.5543 - acc: 0.7986 - val_loss: 0.5391 - val_acc: 0.7993\n",
      "Epoch 6/10\n",
      "1120/1120 [==============================] - 0s 91us/step - loss: 0.5362 - acc: 0.7991 - val_loss: 0.5277 - val_acc: 0.8000\n",
      "Epoch 7/10\n",
      "1120/1120 [==============================] - 0s 135us/step - loss: 0.5284 - acc: 0.7995 - val_loss: 0.5226 - val_acc: 0.8007\n",
      "Epoch 8/10\n",
      "1120/1120 [==============================] - 0s 100us/step - loss: 0.5246 - acc: 0.7995 - val_loss: 0.5197 - val_acc: 0.8007\n",
      "Epoch 9/10\n",
      "1120/1120 [==============================] - 0s 88us/step - loss: 0.5222 - acc: 0.8000 - val_loss: 0.5177 - val_acc: 0.8000\n",
      "Epoch 10/10\n",
      "1120/1120 [==============================] - 0s 102us/step - loss: 0.5202 - acc: 0.8000 - val_loss: 0.5159 - val_acc: 0.8000\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(2500, 8, input_length=maxlen))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train_train, y_train_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. A model using an Embedding layer with Conv1d Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_36 (Embedding)     (None, 500, 128)          320000    \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 494, 32)           28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 98, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 92, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_14 (Glo (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 356,069\n",
      "Trainable params: 356,069\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1120 samples, validate on 280 samples\n",
      "Epoch 1/10\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 2.1677 - acc: 0.5059 - val_loss: 2.1446 - val_acc: 0.5029\n",
      "Epoch 2/10\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 1.9800 - acc: 0.5341 - val_loss: 1.6901 - val_acc: 0.6314\n",
      "Epoch 3/10\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 1.5825 - acc: 0.6395 - val_loss: 1.6871 - val_acc: 0.6314\n",
      "Epoch 4/10\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 1.5789 - acc: 0.6395 - val_loss: 1.6839 - val_acc: 0.6314\n",
      "Epoch 5/10\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 1.5751 - acc: 0.6395 - val_loss: 1.6804 - val_acc: 0.6314\n",
      "Epoch 6/10\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 1.5711 - acc: 0.6395 - val_loss: 1.6767 - val_acc: 0.6314\n",
      "Epoch 7/10\n",
      "1120/1120 [==============================] - 3s 3ms/step - loss: 1.5670 - acc: 0.6395 - val_loss: 1.6728 - val_acc: 0.6314\n",
      "Epoch 8/10\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 1.5623 - acc: 0.6395 - val_loss: 1.6685 - val_acc: 0.6314\n",
      "Epoch 9/10\n",
      "1120/1120 [==============================] - 3s 2ms/step - loss: 1.5573 - acc: 0.6395 - val_loss: 1.6639 - val_acc: 0.6314\n",
      "Epoch 10/10\n",
      "1120/1120 [==============================] - 2s 2ms/step - loss: 1.5517 - acc: 0.6395 - val_loss: 1.6587 - val_acc: 0.6314\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_words, 128, input_length=maxlen))\n",
    "model.add(layers.Conv1D(32, 7, activation='sigmoid')) \n",
    "model.add(layers.MaxPooling1D(5)) #\n",
    "model.add(layers.Conv1D(32, 7, activation='sigmoid'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(5))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train_train, y_train_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. A model using an Embedding layer with one sequential layer (LSTM or GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1120 samples, validate on 280 samples\n",
      "Epoch 1/10\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.6036 - acc: 0.7804 - val_loss: 0.4976 - val_acc: 0.8000\n",
      "Epoch 2/10\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.5003 - acc: 0.8000 - val_loss: 0.5023 - val_acc: 0.8000\n",
      "Epoch 3/10\n",
      "1120/1120 [==============================] - 8s 7ms/step - loss: 0.4954 - acc: 0.8000 - val_loss: 0.4981 - val_acc: 0.8000\n",
      "Epoch 4/10\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.4878 - acc: 0.8000 - val_loss: 0.4879 - val_acc: 0.8000\n",
      "Epoch 5/10\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.4567 - acc: 0.8023 - val_loss: 0.4752 - val_acc: 0.8021\n",
      "Epoch 6/10\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.4376 - acc: 0.8052 - val_loss: 0.4374 - val_acc: 0.8029\n",
      "Epoch 7/10\n",
      "1120/1120 [==============================] - 8s 8ms/step - loss: 0.3852 - acc: 0.8241 - val_loss: 0.3904 - val_acc: 0.8079\n",
      "Epoch 8/10\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.3460 - acc: 0.8420 - val_loss: 0.4012 - val_acc: 0.8321\n",
      "Epoch 9/10\n",
      "1120/1120 [==============================] - 8s 8ms/step - loss: 0.3174 - acc: 0.8734 - val_loss: 0.3434 - val_acc: 0.8336\n",
      "Epoch 10/10\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 0.2740 - acc: 0.9179 - val_loss: 0.3009 - val_acc: 0.8750\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(2500, 32))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train_train, y_train_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. A model using an Embedding layer with stacked sequential layers (LSTM or GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1120 samples, validate on 280 samples\n",
      "Epoch 1/10\n",
      "1120/1120 [==============================] - 37s 33ms/step - loss: 0.5631 - acc: 0.7930 - val_loss: 0.5008 - val_acc: 0.8000\n",
      "Epoch 2/10\n",
      "1120/1120 [==============================] - 34s 30ms/step - loss: 0.5005 - acc: 0.8000 - val_loss: 0.5005 - val_acc: 0.8000\n",
      "Epoch 3/10\n",
      "1120/1120 [==============================] - 34s 30ms/step - loss: 0.4999 - acc: 0.8000 - val_loss: 0.4989 - val_acc: 0.8000\n",
      "Epoch 4/10\n",
      "1120/1120 [==============================] - 34s 30ms/step - loss: 0.5000 - acc: 0.8000 - val_loss: 0.5004 - val_acc: 0.8000\n",
      "Epoch 5/10\n",
      "1120/1120 [==============================] - 31s 28ms/step - loss: 0.4850 - acc: 0.8000 - val_loss: 0.4457 - val_acc: 0.8000\n",
      "Epoch 6/10\n",
      "1120/1120 [==============================] - 31s 28ms/step - loss: 0.4321 - acc: 0.8018 - val_loss: 0.4053 - val_acc: 0.8236\n",
      "Epoch 7/10\n",
      "1120/1120 [==============================] - 31s 27ms/step - loss: 0.3882 - acc: 0.8030 - val_loss: 0.3966 - val_acc: 0.8000\n",
      "Epoch 8/10\n",
      "1120/1120 [==============================] - 30s 27ms/step - loss: 0.3847 - acc: 0.8039 - val_loss: 0.3898 - val_acc: 0.8243\n",
      "Epoch 9/10\n",
      "1120/1120 [==============================] - 30s 27ms/step - loss: 0.3799 - acc: 0.8018 - val_loss: 0.3886 - val_acc: 0.8221\n",
      "Epoch 10/10\n",
      "1120/1120 [==============================] - 30s 27ms/step - loss: 0.3782 - acc: 0.8041 - val_loss: 0.4159 - val_acc: 0.8000\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(2500, 32))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train_train, y_train_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. A model using an Embedding layer with bidirectional sequential layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1120 samples, validate on 280 samples\n",
      "Epoch 1/10\n",
      "1120/1120 [==============================] - 6s 5ms/step - loss: 0.6798 - acc: 0.7241 - val_loss: 0.6603 - val_acc: 0.8000\n",
      "Epoch 2/10\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.6326 - acc: 0.8000 - val_loss: 0.5791 - val_acc: 0.8000\n",
      "Epoch 3/10\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.5281 - acc: 0.8000 - val_loss: 0.5088 - val_acc: 0.8000\n",
      "Epoch 4/10\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.5051 - acc: 0.8000 - val_loss: 0.4986 - val_acc: 0.8000\n",
      "Epoch 5/10\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.4956 - acc: 0.8000 - val_loss: 0.4979 - val_acc: 0.8000\n",
      "Epoch 6/10\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.4939 - acc: 0.8000 - val_loss: 0.4980 - val_acc: 0.8000\n",
      "Epoch 7/10\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.4902 - acc: 0.8000 - val_loss: 0.4963 - val_acc: 0.8000\n",
      "Epoch 8/10\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.4860 - acc: 0.8000 - val_loss: 0.4913 - val_acc: 0.8000\n",
      "Epoch 9/10\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.4782 - acc: 0.8000 - val_loss: 0.4851 - val_acc: 0.8000\n",
      "Epoch 10/10\n",
      "1120/1120 [==============================] - 4s 4ms/step - loss: 0.4578 - acc: 0.8000 - val_loss: 0.4492 - val_acc: 0.8000\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(2500, 32))\n",
    "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
    "model.add(layers.Dense(5, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train_train, y_train_train, epochs=10, batch_size=128, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Now retrain your best model from C, D, and E using dropout (you may need to increase epochs!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_test)\n",
    "sequences = tokenizer.texts_to_sequences(X_test) \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "X_test = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "one_hot_enc = OneHotEncoder()\n",
    "one_hot_enc.fit(y_test.values.reshape(-1, 1))\n",
    "y_test = one_hot_enc.transform(y_test.values.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1120 samples, validate on 280 samples\n",
      "Epoch 1/10\n",
      "1120/1120 [==============================] - 17s 15ms/step - loss: 0.5257 - acc: 0.7900 - val_loss: 0.4968 - val_acc: 0.8000\n",
      "Epoch 2/10\n",
      "1120/1120 [==============================] - 16s 14ms/step - loss: 0.5000 - acc: 0.8000 - val_loss: 0.4994 - val_acc: 0.8000\n",
      "Epoch 3/10\n",
      "1120/1120 [==============================] - 16s 14ms/step - loss: 0.4957 - acc: 0.8000 - val_loss: 0.4899 - val_acc: 0.8000\n",
      "Epoch 4/10\n",
      "1120/1120 [==============================] - 16s 14ms/step - loss: 0.4788 - acc: 0.8011 - val_loss: 0.4462 - val_acc: 0.8086\n",
      "Epoch 5/10\n",
      "1120/1120 [==============================] - 17s 15ms/step - loss: 0.4443 - acc: 0.8125 - val_loss: 0.4342 - val_acc: 0.8186\n",
      "Epoch 6/10\n",
      "1120/1120 [==============================] - 16s 14ms/step - loss: 0.3973 - acc: 0.8307 - val_loss: 0.3812 - val_acc: 0.8393\n",
      "Epoch 7/10\n",
      "1120/1120 [==============================] - 16s 14ms/step - loss: 0.3620 - acc: 0.8443 - val_loss: 0.3636 - val_acc: 0.8436\n",
      "Epoch 8/10\n",
      "1120/1120 [==============================] - 16s 15ms/step - loss: 0.3254 - acc: 0.8605 - val_loss: 0.3707 - val_acc: 0.8321\n",
      "Epoch 9/10\n",
      "1120/1120 [==============================] - 16s 15ms/step - loss: 0.2921 - acc: 0.8804 - val_loss: 0.3125 - val_acc: 0.8693\n",
      "Epoch 10/10\n",
      "1120/1120 [==============================] - 16s 14ms/step - loss: 0.2475 - acc: 0.9005 - val_loss: 0.2941 - val_acc: 0.8857\n",
      "557/557 [==============================] - 1s 2ms/step\n",
      "('Test score:', 0.5706739823736879)\n",
      "('Test accuracy:', 0.779174268245697)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(2500, 32))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2)) \n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train_train, y_train_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=128)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Discuss 1) which model(s) performed best and speculate about 2) how you might try to further improve the predictive power of your model (e.g. Glove embeddings? More layers? Combining Conv1D with LSTM layers? More LSTM hidden nodes?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM model using dropout performed best among all the models. The predictive power may be improved through glove embeddings which provides a suite of pre-trained word embeddings. Glove embeddings measure the similarity of the hidden factors between words to predict their co-occurrence count. It may help improve LSTM model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
